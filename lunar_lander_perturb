#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Mar  9 10:24:22 2020

@author: gwenda

This should use a spiking neural network to play lunar lander using perturbations
"""



"""
LunarLander-v2
Observation , size 8:
min  -1.02 -0.5 -2.3  -2.1 -4.2 -7.3  0  0
max   1.02  1.7  2.3   0.6  3.8  7.3  1  1

"""

import gym
#import numpy as np
#from PIL import Image
import multipurposeNetwork as mpNet
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats


env = gym.make('LunarLander-v2')
env.seed(0)
np.random.seed(0)



def create_input_from_state(state):
    input_spikes = []
    normalized_state = []
    normalized_state.append((state[0]/1.02)*2.5)
    normalized_state.append(((state[1]-.825)/.875)*2.5)
    normalized_state.append((state[2]/2.3)*2.5)
    normalized_state.append(((state[3]+.75)/1.35)*2.5)
    normalized_state.append(((state[4]+.2)/4)*2.5)
    normalized_state.append((state[5]/7.3)*2.5)
    normalized_state.append(state[6]*4 - 2.5)
    normalized_state.append(state[7]*4 - 2.5)

    for i in range(8):
        input_spikes.append(scipy.stats.norm(-2,.5).pdf(normalized_state[i])*2)
        input_spikes.append(scipy.stats.norm(-1,.5).pdf(normalized_state[i])*2)
        input_spikes.append(scipy.stats.norm(0,.5).pdf(normalized_state[i])*2)
        input_spikes.append(scipy.stats.norm(1,.5).pdf(normalized_state[i])*2)
        input_spikes.append(scipy.stats.norm(2,.5).pdf(normalized_state[i])*2)

    
    return [round(i) if i<1 else i for i in input_spikes]

        
        

def make_network(action_space,state_space):  

    lr = 0.001
    spike_amp=1
    slope=10
    axon_len=.1
    leak = None
        
    net = mpNet.Network()
    

    
    neuron_type = {'spiking':spike_amp,'leak':leak,
                           'sigmoid':(40,slope),
                           'learn_delay':1}
    
    neuron_type2 = {'spiking':spike_amp,'leak':leak,
                           'sigmoid':(4,slope),
                           'learn_delay':1}
    

    # we want to make this 40 because you have 5 for each
    input_layer = net.make_group(40,neuron_type)
    
    hidden_layer = net.make_group(7,neuron_type)
    hidden_layer2 = net.make_group(7,neuron_type2)
    
    output_layer = net.make_empty_group()
    
    # make the four output neurons
    
    
    net.add_neuron(neuron_type=neuron_type2,
                   neuron_group=output_layer,neuron_id=2,is_output=True)
    
    net.add_neuron(neuron_type=neuron_type2,
                   neuron_group=output_layer,neuron_id=3,is_output=True)
    
    net.add_neuron(neuron_type=neuron_type2,
                   neuron_group=output_layer,neuron_id=1,is_output=True)
    
    net.add_neuron(neuron_type=neuron_type2,
                   neuron_group=output_layer,neuron_id=0,is_output=True)
    

    
    # forward connections
    
    net.connect_groups(input_layer,hidden_layer,axon_len,
                       learn_type=None,lr=lr)
    
    
    net.connect_groups(hidden_layer,hidden_layer2,axon_len,
                       learn_type=None,lr=lr)
    
    
    net.connect_groups(hidden_layer2,output_layer,axon_len,
                       learn_type=None,lr=lr)
    

    
    return net


            
def run_network(episode):
    
    
    loss = [-200]
    agent = make_network(env.action_space.n, env.observation_space.shape[0])
    
    input_layer = agent.neuron_groups[1]

    
    
    for e in range(episode):
        agent.reset(perturb=True)
        
        state = env.reset()
        state = np.reshape(state, (1, 8))
        score = 0
        max_steps = 3000
        time = 0
        for i in range(max_steps):
            
            spikes = create_input_from_state(state[0])
            
            agent.fire_neurons(input_layer,spikes,time)

        
            action = agent.run(time+1)

            
            env.render()
            
            next_state, reward, done, _ = env.step(action)
            
            score += reward
            
            
                                    
            state = np.reshape(next_state, (1, 8))
                        
            if done:
                print("episode: {}/{}, score: {}".format(e, episode, score))
                break
            
            time += 1
        

        dopamine = (score-max(loss[-100:]))/10.0

        if dopamine > 2:
            dopamine = 2
            
        #print("dopamine is {}".format(dopamine))
        
        agent.global_dopamine_flood(dopamine)
        #agent.run(6000)
        
        
        loss.append(score)
       

        # Average score of last 100 episode
        is_solved = np.mean(loss[-100:])
        if is_solved > 200:
            print('\n Task Completed! \n')
            break
        print("Average over last 100 episode: {0:.2f} \n".format(is_solved))
        
    return loss



if __name__ == '__main__':
    
    print(env.observation_space)
    print(env.action_space)
    episodes = 4000
    loss = run_network(episodes)
    env.close()
    plt.plot([i+1 for i in range(0, len(loss), 2)], loss[::2])
    plt.show()
